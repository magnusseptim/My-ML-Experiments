{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version, may contain significant errors and examples of extremal incompetence of the autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_data_folder = 'data/languages/paragraphs/'\n",
    "dataset = load_files(languages_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.DataFrame(data = dataset.target, columns = ['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset.target:\n",
    "    temp.append(dataset.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = zip(map(lambda x: x.decode('utf8'),dataset.data), temp, dataset.target),columns = ['text','lang', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contrariamente, proyectos como Wikipedia, Susn...</td>\n",
       "      <td>es</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Para todos los ejemplos citados y sus variante...</td>\n",
       "      <td>es</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sie lag im September 2018 auf dem fünften Plat...</td>\n",
       "      <td>de</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Según la tesis doctoral de Ortega Soto (2009) ...</td>\n",
       "      <td>es</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>モバイルサイトの構築と平行して2012年からAndroid・iOS・Windows・Fire...</td>\n",
       "      <td>ja</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang  target\n",
       "0  Contrariamente, proyectos como Wikipedia, Susn...   es       3\n",
       "1  Para todos los ejemplos citados y sus variante...   es       3\n",
       "2  Sie lag im September 2018 auf dem fünften Plat...   de       1\n",
       "3  Según la tesis doctoral de Ortega Soto (2009) ...   es       3\n",
       "4  モバイルサイトの構築と平行して2012年からAndroid・iOS・Windows・Fire...   ja       6"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790,)"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198,)"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790,)"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198,)"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47     A Esplanada é o principal local de interação e...\n",
       "697    Da sich Wikipedia auf Enzyklopädie-Artikel bes...\n",
       "660    En juillet 2007, Wikipédia a fait l'objet d'un...\n",
       "746    Em janeiro de 2007, a Wikipédia entrou na list...\n",
       "964    Bisher haben international mehr als 2,0 Millio...\n",
       "59     Un contributeur est classé « actif » tant qu'i...\n",
       "378    Ende 2000/Anfang 2001 wurden Sanger und Wales ...\n",
       "870    In certain cases, all editors are allowed to s...\n",
       "5      В 2009 году художник Роб Мэттьюс (англ. Rob Ma...\n",
       "25     2009年11月、ウォール・ストリート・ジャーナルは、ウィキペディアへの訪問者が増加する一方...\n",
       "545    Op 17 juni 2015 werd de jaarlijkse Prinses van...\n",
       "206    Researchers from Washington University develop...\n",
       "611    2005年から2006年の年頭にかけてウィキペディアの閲覧者は飛躍的に増加し、インターネット...\n",
       "62     La croissance de Wikipédia a été favorisée par...\n",
       "859    Obvious vandalism is generally easy to remove ...\n",
       "462    Bei umstrittenen Entscheidungen wird in der Wi...\n",
       "215    Alla fine del 2005 è sorta una controversia do...\n",
       "41     In February 2007, an article in The Harvard Cr...\n",
       "364    Mit dem Speichern ihrer Bearbeitung geben die ...\n",
       "167    Per questo Wikipedia è stata usata dai media, ...\n",
       "875    Etimologicamente, Wikipedia significa \"cultura...\n",
       "519    Entspricht ein Artikel nicht den Relevanzkrite...\n",
       "23     La cultura de la sociedad ha variado, según el...\n",
       "58     29 kwietnia 2017 dostęp do Wikipedii, bez poda...\n",
       "278    Though the various language editions are held ...\n",
       "373    Anche il concetto di \"enciclopedia\" di Wikiped...\n",
       "224    Para editar un artículo se recurre al código w...\n",
       "807    On September 16, 2007, The Washington Post rep...\n",
       "210    في عام 2016 في مؤسسة ويكيبيديا التعليمية  ومؤس...\n",
       "411    Der ökonomische Wert von Wikipedia wird auf 3,...\n",
       "                             ...                        \n",
       "112    2003年11月、ロシア語版ウィキペディアでライセンス形態についての論争が基となり、一部の利...\n",
       "730    Dans les pays pauvres, là où le livre est rare...\n",
       "413    Todo o texto contido na Wikipedia era coberto ...\n",
       "428    Mehrere Unternehmen und Organisationen boten d...\n",
       "749    時として悪意を持って虚偽の情報を記載したり、不適切な言葉を書き連ねたり、ページを白紙化するな...\n",
       "281    Artikelen op Wikipedia scoren over het algemee...\n",
       "74     Contribuintes, registrados ou não, podem tirar...\n",
       "893    Content in Wikipedia is subject to the laws (i...\n",
       "32     Jimmy Wales descreve a Wikipédia como \"um esfo...\n",
       "823    Community-produced news publications include t...\n",
       "96     Several MediaWiki extensions are installed[228...\n",
       "322    Desde su nacimiento, Larry Sanger ya discrepó ...\n",
       "121    メタ・ウィキメディアとはウィキメディア財団（後述）のプロジェクト全体を扱うウェブサイトであり...\n",
       "294    「ウィキペディアはウェールズを「寛大な独裁者」として戴くコミュニティ自治の集団である」、と形...\n",
       "65     Wikipedia, debido a su condición de encicloped...\n",
       "397    В дополнение к логистическому росту количества...\n",
       "831    عبارة: نص ضعيف بمصدر، أفضل من كتابة جيدة غير م...\n",
       "506    Le suivi des consultations des articles de Wik...\n",
       "907    Am 15. Januar 2011 feierte die Wikipedia ihr z...\n",
       "669    Der Inhalt aller Seiten ist als Hypertext orga...\n",
       "589    Wikipédia peut aussi être consultée grâce à l'...\n",
       "940    In February 2014, The New York Times reported ...\n",
       "152    Википедия получает от 25 000 до 60 000 запросо...\n",
       "661    De las 300 ediciones, quince superan el 1 000 ...\n",
       "485    著作権法や著作権についてのウィキペディアの方針に対する理解不足から、記事を書く際に書籍やネッ...\n",
       "529    These numbers refer only to the quantity of ar...\n",
       "468    La rivista scientifica Nature nel 2005 riporta...\n",
       "237    On September 28, 2007, Italian politician Fran...\n",
       "700    In mei 2004 ontving Wikipedia de Golden Nica v...\n",
       "2      Sie lag im September 2018 auf dem fünften Plat...\n",
       "Name: text, Length: 790, dtype: object"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "es\n",
      "de\n",
      "es\n",
      "ja\n",
      "ru\n",
      "nl\n",
      "fr\n",
      "fr\n",
      "pl\n",
      "nl\n",
      "fr\n",
      "it\n",
      "en\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.target[:15]:\n",
    "    print(dataset.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(items):\n",
    "    co_vect = CountVectorizer(analyzer='word')\n",
    "    train_counts = co_vect.fit_transform(items)\n",
    "    tf_transformer = TfidfTransformer(use_idf=True).fit(train_counts)\n",
    "    train_tfidf = tf_transformer.transform(train_counts)\n",
    "    return (train_tfidf,co_vect,tf_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = create_tfidf(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(process[0],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = process[1].transform(docs_test)\n",
    "X_new_tfidf = process[2].transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ar       1.00      0.17      0.29         6\n",
      "          de       0.45      1.00      0.62        20\n",
      "          en       0.87      1.00      0.93        27\n",
      "          es       1.00      0.97      0.98        32\n",
      "          fr       1.00      1.00      1.00        25\n",
      "          it       1.00      1.00      1.00        14\n",
      "          ja       1.00      0.06      0.11        17\n",
      "          nl       1.00      1.00      1.00        11\n",
      "          pl       1.00      0.45      0.62        11\n",
      "          pt       1.00      1.00      1.00        23\n",
      "          ru       1.00      1.00      1.00        12\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       198\n",
      "   macro avg       0.94      0.79      0.78       198\n",
      "weighted avg       0.93      0.86      0.83       198\n",
      "\n",
      "[[ 1  5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 27  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 31  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 25  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0  0]\n",
      " [ 0 15  1  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0  0]\n",
      " [ 0  4  2  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 23  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(y_test, predicted, target_names=dataset.target_names))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language of \"This is a language detection test.\" is \"en\"\n",
      "The language of \"Ceci est un test de détection de la langue.\" is \"fr\"\n",
      "The language of \"Dies ist ein Test, um die Sprache zu erkennen.\" is \"de\"\n",
      "The language of \"Sprachen Sie\" is \"de\"\n",
      "The language of \"Zuppa di funghi\" is \"it\"\n"
     ]
    }
   ],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    'This is a language detection test.',\n",
    "    'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "    'Sprachen Sie',\n",
    "    'Zuppa di funghi'\n",
    "]\n",
    "se_df = pd.DataFrame(sentences, columns = ['text'])\n",
    "\n",
    "se_counts = process[1].transform(se_df['text'])\n",
    "se_tfidf = process[2].transform(se_counts)\n",
    "\n",
    "predicted = clf.predict(se_tfidf)\n",
    "\n",
    "for s, p in zip(se_df['text'], predicted):\n",
    "    print('The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(docs_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ar       1.00      0.17      0.29         6\n",
      "          de       0.45      1.00      0.62        20\n",
      "          en       0.87      1.00      0.93        27\n",
      "          es       1.00      0.97      0.98        32\n",
      "          fr       1.00      1.00      1.00        25\n",
      "          it       1.00      1.00      1.00        14\n",
      "          ja       1.00      0.06      0.11        17\n",
      "          nl       1.00      1.00      1.00        11\n",
      "          pl       1.00      0.45      0.62        11\n",
      "          pt       1.00      1.00      1.00        23\n",
      "          ru       1.00      1.00      1.00        12\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       198\n",
      "   macro avg       0.94      0.79      0.78       198\n",
      "weighted avg       0.93      0.86      0.83       198\n",
      "\n",
      "[[ 1  5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 27  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 31  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 25  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0  0]\n",
      " [ 0 15  1  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0  0]\n",
      " [ 0  4  2  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 23  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language of \"This is a language detection test.\" is \"en\"\n",
      "The language of \"Ceci est un test de détection de la langue.\" is \"fr\"\n",
      "The language of \"Dies ist ein Test, um die Sprache zu erkennen.\" is \"de\"\n",
      "The language of \"Sprachen Sie\" is \"de\"\n",
      "The language of \"Zuppa di funghi\" is \"it\"\n"
     ]
    }
   ],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    'This is a language detection test.',\n",
    "    'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "    'Sprachen Sie',\n",
    "    'Zuppa di funghi'\n",
    "]\n",
    "predicted = text_clf.predict(sentences)\n",
    "for s, p in zip(sentences, predicted):\n",
    "    print('The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
